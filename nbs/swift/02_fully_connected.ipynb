{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/jmd/workspace/ml/fastai/nbs/swift/FastaiNotebook_01a_fastai_layers\")\n",
      "\t\tFastaiNotebook_01a_fastai_layers\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpejz8j8mm/swift-install\n",
      "/home/jmd/swift/usr/bin/swift-build: /home/jmd/anaconda3/lib/libcurl.so.4: no version information available (required by /home/jmd/swift/usr/lib/swift/linux/libFoundationNetworking.so)\n",
      "Updating https://github.com/saeta/Just\n",
      "Updating https://github.com/latenitesoft/NotebookExport\n",
      "Updating https://github.com/mxcl/Path.swift\n",
      "[1/6] Compiling FastaiNotebook_01a_fastai_layers 01_matmul.swift\n",
      "[2/6] Compiling FastaiNotebook_01a_fastai_layers 00_load_data.swift\n",
      "[3/6] Compiling FastaiNotebook_01a_fastai_layers 01a_fastai_layers.swift\n",
      "[4/7] Merging module FastaiNotebook_01a_fastai_layers\n",
      "[5/8] Compiling jupyterInstalledPackages jupyterInstalledPackages.swift\n",
      "[6/9] Wrapping AST for FastaiNotebook_01a_fastai_layers for debugging\n",
      "[7/9] Merging module jupyterInstalledPackages\n",
      "[8/8] Linking libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_01a_fastai_layers\")' FastaiNotebook_01a_fastai_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_01a_fastai_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public typealias TF=Tensor<Float>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func normalize(_ x:TF, mean:TF, std:TF) -> TF {\n",
    "    return (x-mean)/std\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13066047 0.3081079\r\n"
     ]
    }
   ],
   "source": [
    "let trainMean = xTrain.mean()\n",
    "let trainStd  = xTrain.standardDeviation()\n",
    "print(trainMean, trainStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = normalize(xTrain, mean: trainMean, std: trainStd)\n",
    "xValid = normalize(xValid, mean: trainMean, std: trainStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func testNearZero(_ a: TF, tolerance: Float = 1e-3) {\n",
    "    assert((abs(a) .< tolerance).all(), \"Near zero: \\(a)\")\n",
    "}\n",
    "\n",
    "public func testSame(_ a: TF, _ b: TF) {\n",
    "    // Check shapes match so broadcasting doesn't hide shape errors.\n",
    "    assert(a.shape == b.shape)\n",
    "    testNearZero(a-b)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNearZero(xTrain.mean())\n",
    "testNearZero(xTrain.standardDeviation() - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 10\r\n"
     ]
    }
   ],
   "source": [
    "let (n,m) = (xTrain.shape[0],xTrain.shape[1])\n",
    "let c = yTrain.max()+1\n",
    "print(n, m, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "//num hidden\n",
    "let nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "// simplified kaiming init / he init\n",
    "let w1 = TF(randomNormal: [m, nh]) / sqrt(Float(m))\n",
    "let b1 = TF(zeros: [nh])\n",
    "let w2 = TF(randomNormal: [nh,1]) / sqrt(Float(nh))\n",
    "let b2 = TF(zeros: [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNearZero(w1.mean())\n",
    "testNearZero(w1.standardDeviation()-1/sqrt(Float(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0060177343 1.0076997\r\n"
     ]
    }
   ],
   "source": [
    "// This should be ~ (0,1) (mean,std)...\n",
    "print(xValid.mean(), xValid.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "func lin(_ x: TF, _ w: TF, _ b: TF) -> TF { return x•w+b }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = lin(xValid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.087955624 0.9942245\r\n"
     ]
    }
   ],
   "source": [
    "//...so should this, because we used kaiming init, which is designed to do this\n",
    "print(t.mean(), t.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "func myRelu(_ x:TF) -> TF { return max(x, 0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = myRelu(lin(xValid, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43534523 0.6163497\r\n"
     ]
    }
   ],
   "source": [
    "//...actually it really should be this!\n",
    "print(t.mean(),t.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "// kaiming init / he init for relu\n",
    "let w1 = TF(randomNormal: [m,nh]) * sqrt(2.0/Float(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00022416159 0.05053568\r\n"
     ]
    }
   ],
   "source": [
    "print(w1.mean(), w1.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46108657 0.7441583\r\n"
     ]
    }
   ],
   "source": [
    "let t = myRelu(lin(xValid, w1, b1))\n",
    "print(t.mean(), t.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "func model(_ xb: TF) -> TF {\n",
    "    let l1 = lin(xb, w1, b1)\n",
    "    let l2 = myRelu(l1)\n",
    "    let l3 = lin(l2, w2, b2)\n",
    "    return l3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 0.4939547 ms,   min: 0.456631 ms,   max: 0.567361 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) { _ = model(xValid) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "let preds = model(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func mse(_ out: TF, _ targ: TF) -> TF {\n",
    "    return (out.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert these to Float dtype.\n",
    "var yTrainF = TF(yTrain)\n",
    "var yValidF = TF(yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.051638\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, yTrainF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// WARNING: This is designed to be similar to the PyTorch 02_fully_connected lesson,\n",
    "/// this isn't idiomatic Swift code.\n",
    "class TFGrad {\n",
    "    var inner, grad:  TF\n",
    "    \n",
    "    init(_ x: TF) {\n",
    "        inner = x\n",
    "        grad = TF(zeros: x.shape)\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Redefine our functions on TFGrad.\n",
    "func lin(_ x: TFGrad, _ w: TFGrad, _ b: TFGrad) -> TFGrad {\n",
    "    return TFGrad(x.inner • w.inner + b.inner)\n",
    "}\n",
    "func myRelu(_ x: TFGrad) -> TFGrad {\n",
    "    return TFGrad(max(x.inner, 0))\n",
    "}\n",
    "func mse(_ inp: TFGrad, _ targ: TF) -> TF {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    return (inp.inner.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define our gradient functions.\n",
    "func mseGrad(_ inp: TFGrad, _ targ: TF) {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    inp.grad = 2.0 * (inp.inner.squeezingShape(at: -1) - targ).expandingShape(at: -1) / Float(inp.inner.shape[0])\n",
    "}\n",
    "\n",
    "func reluGrad(_ inp: TFGrad, _ out: TFGrad) {\n",
    "    //grad of relu with respect to input activations\n",
    "    inp.grad = out.grad.replacing(with: TF(zeros: inp.inner.shape), where: (inp.inner .< 0))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "func linGrad(_ inp:TFGrad, _ out:TFGrad, _ w:TFGrad, _ b:TFGrad){\n",
    "    // grad of linear layer with respect to input activations, weights and bias\n",
    "    inp.grad = out.grad • w.inner.transposed()\n",
    "    w.grad = inp.inner.transposed() • out.grad\n",
    "    b.grad = out.grad.sum(squeezingAxes: 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "let w1a = TFGrad(w1)\n",
    "let b1a = TFGrad(b1)\n",
    "let w2a = TFGrad(w2)\n",
    "let b2a = TFGrad(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp:TFGrad, _ targ:TF){\n",
    "    // forward pass:\n",
    "    let l1 = lin(inp, w1a, b1a)\n",
    "    let l2 = myRelu(l1)\n",
    "    let out = lin(l2, w2a, b2a)\n",
    "    //we don't actually need the loss in backward!\n",
    "    let loss = mse(out, targ)\n",
    "    \n",
    "    // backward pass:\n",
    "    mseGrad(out, targ)\n",
    "    linGrad(l2, out, w2a, b2a)\n",
    "    reluGrad(l1, l2)\n",
    "    linGrad(inp, l1, w1a, b1a)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "let inp = TFGrad(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "forwardAndBackward(inp, yTrainF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo(a:b:)\r\n",
      "bar(x:)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "731\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This function prints out the calling function's name.  This \n",
    "// is useful to see what is going on in your program..\n",
    "func trace(function: String = #function) {\n",
    "  print(function)\n",
    "}\n",
    "\n",
    "// Try out the trace helper function.\n",
    "func foo(a: Int, b: Int) -> Int {\n",
    "  trace()\n",
    "  return a+b\n",
    "}\n",
    "func bar(x: Int) -> Int {\n",
    "  trace()\n",
    "  return x*42+17\n",
    "}\n",
    "\n",
    "foo(a: 1, b: 2)\n",
    "bar(x: 17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "func square(_ x: TF) -> TF {\n",
    "    trace() \n",
    "    return x * x\n",
    "}\n",
    "func 𝛁square(_ x: TF) -> TF {\n",
    "    trace()\n",
    "    return 2 * x\n",
    "}\n",
    "\n",
    "func mean(_ x: TF) -> TF {\n",
    "    trace()\n",
    "    return x.mean()  // this is a reduction.  (can someone write this out longhand?)\n",
    "}\n",
    "func 𝛁mean(_ x: TF) -> TF {\n",
    "    trace()\n",
    "    return TF(ones: x.shape) / Float(x.shape[0])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "func mseInner(_ x: TF) -> TF {\n",
    "    return mean(square(x))\n",
    "}\n",
    "\n",
    "func 𝛁mseInner(_ x: TF) -> TF {\n",
    "    return 𝛁mean(square(x)) * 𝛁square(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square(_:)\n",
      "mean(_:)\n",
      "square(_:)\n",
      "𝛁mean(_:)\n",
      "𝛁square(_:)\n",
      "\n",
      "result: 7.5\n",
      "gradient: [0.5, 1.0, 1.5, 2.0]\n"
     ]
    }
   ],
   "source": [
    "func mseInnerAndGrad(_ x: TF) -> (TF, TF) {\n",
    "  return (mseInner(x), 𝛁mseInner(x))    \n",
    "}\n",
    "\n",
    "let exampleData = TF([1, 2, 3, 4])\n",
    "\n",
    "let (mseInnerResult1, mseInnerGrad1) = mseInnerAndGrad(exampleData)\n",
    "print()\n",
    "\n",
    "print(\"result:\", mseInnerResult1)\n",
    "print(\"gradient:\", mseInnerGrad1)\n",
    "\n",
    "// Check that our gradient matches builtin S4TF's autodiff.\n",
    "let builtinGrad = gradient(at: exampleData) { x in (x*x).mean() }\n",
    "testSame(mseInnerGrad1, builtinGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The chainer for the gradient of square(x).\n",
    "func square𝛁Chain(x: TF, ddx: TF) -> TF {\n",
    "  trace()\n",
    "  return ddx * 2*x\n",
    "}\n",
    "\n",
    "// The chainer for the gradient of mean(x).\n",
    "func mean𝛁Chain(x: TF, ddx: TF) -> TF {\n",
    "  trace()\n",
    "  return ddx * TF(ones: x.shape) / Float(x.shape[0])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Returns x*x and the chain for the gradient of x*x.\n",
    "func squareVWC(_ x: TF) -> (value: TF, \n",
    "                            chain: (TF) -> TF) {\n",
    "  trace()\n",
    "  return (value: x*x,\n",
    "          chain: { ddx in square𝛁Chain(x: x, ddx: ddx) })    \n",
    "}\n",
    "\n",
    "// Returns the mean of x and the chain for the mean.\n",
    "func meanVWC(_ x: TF) -> (value: TF,\n",
    "                          chain: (TF) -> TF) {\n",
    "  trace()\n",
    "  return (value: x.mean(),\n",
    "          chain: { ddx in mean𝛁Chain(x: x, ddx: ddx) })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We implement mean(square(x)) by calling each of the VWCs in turn.\n",
    "func mseInnerVWC(_ x: TF) -> (value: TF, \n",
    "                              chain: (TF) -> TF) {\n",
    "\n",
    "  // square and mean are tuples that carry the value/chain for each step.\n",
    "  let square = squareVWC(x)\n",
    "  let mean   = meanVWC(square.value)\n",
    "\n",
    "  // The result is the combination of the results and the pullbacks.\n",
    "  return (mean.value,\n",
    "          // The mseInner pullback calls the functions in reverse order.\n",
    "          { v in square.chain(mean.chain(v)) })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling the forward function:\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "\n",
      "Calling the backward function:\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "\n",
      "[0.5, 1.0, 1.5, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Calling the forward function:\")\n",
    "let mseInner2 = mseInnerVWC(exampleData)\n",
    "print()\n",
    "\n",
    "testSame(mseInner2.value, mseInnerResult1)\n",
    "\n",
    "\n",
    "print(\"Calling the backward function:\")\n",
    "let mseInnerGrad2 = mseInner2.chain(TF(1))\n",
    "print()\n",
    "\n",
    "print(mseInnerGrad2)\n",
    "// Check that we get the same result.\n",
    "testSame(mseInnerGrad2, builtinGrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "func reluVWC(_ x: TF) -> (value: TF, chain: (TF) -> TF) {\n",
    "    return (value: max(x, 0),\n",
    "            // Pullback for max(x, 0)\n",
    "            chain: { 𝛁out -> TF in\n",
    "              𝛁out.replacing(with: TF(zeros: x.shape), where: x .< 0)\n",
    "            })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "func linVWC(_ inp: TF, _ w: TF, _ b: TF) -> (value: TF,\n",
    "                                             chain: (TF) -> (TF, TF, TF)) {\n",
    "    return (value: inp • w + b,\n",
    "            // Pullback for inp • w + b.  Three results because 'lin' has three args.\n",
    "            chain: { 𝛁out in\n",
    "              (𝛁out • w.transposed(), \n",
    "               inp.transposed() • 𝛁out,\n",
    "               𝛁out.unbroadcasted(to: b.shape))\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "func mseVWC(_ inp: TF, _ targ: TF) -> (value: TF,\n",
    "                                       chain: (TF) -> (TF)) {\n",
    "    let tmp = inp.squeezingShape(at: -1) - targ\n",
    "    \n",
    "    // We already wrote a VWC for x.square().mean(), so we can reuse it.\n",
    "    let mseInner = mseInnerVWC(tmp)\n",
    "    \n",
    "    // Return the result, and a pullback that expands back out to\n",
    "    // the input shape.\n",
    "    return (mseInner.value, \n",
    "            { v in mseInner.chain(v).expandingShape(at: -1) })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp: TF, _ targ: TF) -> (TF, TF, TF, TF, TF) {\n",
    "    // Forward pass:\n",
    "    let l1   = linVWC(inp, w1, b1)\n",
    "    let l2   = reluVWC(l1.value)\n",
    "    let out  = linVWC(l2.value, w2, b2)\n",
    "    //we don't actually need the loss in backward, but we need the pullback.\n",
    "    let loss = mseVWC(out.value, targ)\n",
    "    \n",
    "    // Backward pass:\n",
    "    let 𝛁loss = TF(1) // We don't really need it but the gradient of the loss with respect to itself is 1\n",
    "    let 𝛁out = loss.chain(𝛁loss)\n",
    "    let (𝛁l2, 𝛁w2, 𝛁b2) = out.chain(𝛁out)\n",
    "    let 𝛁l1 = l2.chain(𝛁l2)\n",
    "    let (𝛁inp, 𝛁w1, 𝛁b1) = l1.chain(𝛁l1)\n",
    "    return (𝛁inp, 𝛁w1, 𝛁b1, 𝛁w2, 𝛁b2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squareVWC(_:)\r\n",
      "meanVWC(_:)\r\n",
      "mean𝛁Chain(x:ddx:)\r\n",
      "square𝛁Chain(x:ddx:)\r\n"
     ]
    }
   ],
   "source": [
    "let (𝛁xTrain, 𝛁w1, 𝛁b1, 𝛁w2, 𝛁b2) = forwardAndBackward(xTrain, yTrainF)\n",
    "// Check this is still all correct\n",
    "testSame(inp.grad, 𝛁xTrain)\n",
    "testSame(w1a.grad, 𝛁w1)\n",
    "testSame(b1a.grad, 𝛁b1)\n",
    "testSame(w2a.grad, 𝛁w2)\n",
    "testSame(b2a.grad, 𝛁b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor<Float>) -> Tensor<Float>\r\n"
     ]
    }
   ],
   "source": [
    "@differentiable\n",
    "func mseInnerForAD(_ x: TF) -> TF {\n",
    "    return x.squared().mean()\n",
    "}\n",
    "\n",
    "let mseInner𝛁Chain = pullback(at: exampleData, in: mseInnerForAD)\n",
    "print(type(of: mseInner𝛁Chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 7.5, grad:  [0.5, 1.0, 1.5, 2.0]\r\n"
     ]
    }
   ],
   "source": [
    "let (value, grad) = valueWithGradient(at: exampleData, in: mseInnerForAD)\n",
    "\n",
    "print(\"value: \\(value), grad:  \\(grad)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 1.0, 1.5, 2.0]\n"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(at: exampleData) { ($0*$0).mean() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@differentiable\n",
    "func forward(_ inp: TF, _ targ: TF, w1: TF, b1: TF, w2: TF, b2: TF) -> TF {\n",
    "    // FIXME: use lin\n",
    "    let l1 = matmul(inp, w1) + b1\n",
    "    let l2 = relu(l1)\n",
    "    let l3 = matmul(l2, w2) + b2\n",
    "    return (l3.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MyModel: Differentiable {\n",
    "    public var w1, b1, w2, b2: TF\n",
    "}\n",
    "\n",
    "// Create an instance of our model with all the individual parameters we initialized.\n",
    "let model = MyModel(w1: w1, b1: b1, w2: w2, b2: b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension MyModel {\n",
    "    @differentiable\n",
    "    func forward(_ input: TF, _ target: TF) -> TF {\n",
    "        // FIXME: use lin\n",
    "        let l1 = matmul(input, w1) + b1\n",
    "        let l2 = relu(l1)\n",
    "        let l3 = matmul(l2, w2) + b2\n",
    "        // use mse\n",
    "        return (l3.squeezingShape(at: -1) - target).squared().mean()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Grads is a struct with one gradient per parameter.\n",
    "let grads = gradient(at: model) { model in model.forward(xTrain, yTrainF) }\n",
    "\n",
    "// Check that this still calculates the same thing.\n",
    "testSame(𝛁w1, grads.w1)\n",
    "testSame(𝛁b1, grads.b1)\n",
    "testSame(𝛁w2, grads.w2)\n",
    "testSame(𝛁b2, grads.b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "squareVWC(_:)\n",
      "meanVWC(_:)\n",
      "mean𝛁Chain(x:ddx:)\n",
      "square𝛁Chain(x:ddx:)\n",
      "average: 16.4167796 ms,   min: 12.753511 ms,   max: 19.997692 ms\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) { _ = forwardAndBackward(xTrain, yTrainF) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 9.8112337 ms,   min: 8.089194 ms,   max: 12.696444 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) {\n",
    "    _ = valueWithGradient(at: model) { \n",
    "        model in model.forward(xTrain, yTrainF)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\r\n"
     ]
    }
   ],
   "source": [
    "import NotebookExport\n",
    "let exporter = NotebookExport(Path.cwd/\"02_fully_connected.ipynb\")\n",
    "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
